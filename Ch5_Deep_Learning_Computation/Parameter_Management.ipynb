{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ultimate goal of training deep networks is to find good parameter values for a given architecture. When everything is standard, the __torch.nn.Sequential__ class is a perfectly good tool for it. However, very few models are entirely standard and most scientists want to build things that are novel. This section shows how to manipulate parameters. In particular we will cover the following aspects:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accessing parameters for debugging, diagnostics, to visualize them or to save them is the first step to understanding how to work with custom models.\n",
    "- Secondly, we want to set them in specific ways, e.g. for initialization purposes. We discuss the structure of parameter initializers.\n",
    "- Lastly, we show how this knowledge can be put to good use by building networks that share some parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we start from our trusty Multilayer Perceptron with a hidden layer. This will serve as our choice for demonstrating the various features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshit Mittal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6711,  0.4682,  0.1206, -0.1714,  0.6909,  0.0103, -0.3668,  0.2536,\n",
       "         -0.2216,  0.2312],\n",
       "        [ 0.1162, -0.1884,  0.0085, -0.4268, -0.0854, -0.1953, -0.4664, -0.3036,\n",
       "         -0.2258,  0.0847]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add_module('Linear_1', nn.Linear(20, 256, bias = False))\n",
    "net.add_module('relu', nn.ReLU())\n",
    "net.add_module('Linear_2', nn.Linear(256, 10, bias = False))\n",
    "\n",
    "# the init_weights function initializes the weights of our multi-layer perceptron \n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "\n",
    "# the net.apply() applies the above stated initialization of weights to our net        \n",
    "\n",
    "net.apply(init_weights) \n",
    "\n",
    "x = torch.randn(2,20)   #initialing a random tensor of shape (2,20)\n",
    "net(x)  #Forward computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Access\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of a Sequential class we can access the parameters with ease, simply by calling __net.parameters__. Let’s try this out in practice by inspecting the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Sequential(\n",
       "  (Linear_1): Linear(in_features=20, out_features=256, bias=False)\n",
       "  (relu): ReLU()\n",
       "  (Linear_2): Linear(in_features=256, out_features=10, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output tells us a number of things. Firstly, there are 3 layers; 2 linear layers and 1 ReLU layer as we would expect. The output also specifies the shapes that we would expect from linear layers. In particular the names of the parameters are very useful since they allow us to identify parameters uniquely even in a network of hundreds of layers and with nontrivial structure. Also, output also tells us that bias is __False__ as we specified it.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targeted Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do something useful with the parameters we need to access them, though. There are several ways to do this, ranging from simple to general. Let’s look at some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(net[0].bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns the bias of the first linear layer. Since we initialized the bias to be __False__, the output is None. We can also access the parameters by name, such as _**Linear_1**_. Both methods are entirely equivalent but the first method leads to much more readable code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0538,  0.0277, -0.0919,  ...,  0.1462,  0.0200,  0.0782],\n",
      "        [-0.0448,  0.0143,  0.1370,  ...,  0.1414,  0.0789,  0.0143],\n",
      "        [ 0.1147,  0.1246, -0.0902,  ..., -0.1053, -0.1316,  0.0570],\n",
      "        ...,\n",
      "        [ 0.0820, -0.0803,  0.0340,  ..., -0.0170,  0.1419, -0.0437],\n",
      "        [-0.0375,  0.0196, -0.0634,  ..., -0.0050,  0.1458,  0.0978],\n",
      "        [ 0.0128,  0.0569,  0.0877,  ...,  0.0289,  0.1298,  0.0806]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0538,  0.0277, -0.0919,  ...,  0.1462,  0.0200,  0.0782],\n",
      "        [-0.0448,  0.0143,  0.1370,  ...,  0.1414,  0.0789,  0.0143],\n",
      "        [ 0.1147,  0.1246, -0.0902,  ..., -0.1053, -0.1316,  0.0570],\n",
      "        ...,\n",
      "        [ 0.0820, -0.0803,  0.0340,  ..., -0.0170,  0.1419, -0.0437],\n",
      "        [-0.0375,  0.0196, -0.0634,  ..., -0.0050,  0.1458,  0.0978],\n",
      "        [ 0.0128,  0.0569,  0.0877,  ...,  0.0289,  0.1298,  0.0806]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(net.Linear_1.weight)\n",
    "print(net[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the weights are nonzero. This is by design since we applied __Xavier initialization__ to our network. We can also compute the gradient with respect to the parameters. It has the same shape as the weight. However, since we did not invoke backpropagation yet, the output is None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(net[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All parameters at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing parameters as described above can be a bit tedious, in particular if we have more complex blocks, or blocks of blocks (or even blocks of blocks of blocks), since we need to walk through the entire tree in reverse order to how the blocks were constructed. To avoid this, blocks come with a method __state_dict__ which grabs all parameters of a network in one dictionary such that we can traverse it with ease. It does so by iterating over all constituents of a block and calls __state_dict__ on subblocks as needed. To see the difference consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.state_dict of Linear(in_features=20, out_features=256, bias=False)>\n",
      "<bound method Module.state_dict of Sequential(\n",
      "  (Linear_1): Linear(in_features=20, out_features=256, bias=False)\n",
      "  (relu): ReLU()\n",
      "  (Linear_2): Linear(in_features=256, out_features=10, bias=False)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(net[0].state_dict) # parameters only for first layer\n",
    "print(net.state_dict) # parameters for entire network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides us with a third way of accessing the parameters of the network. If we wanted to get the value of the weight term of the second linear layer we could simply use this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0538,  0.0277, -0.0919,  ...,  0.1462,  0.0200,  0.0782],\n",
       "        [-0.0448,  0.0143,  0.1370,  ...,  0.1414,  0.0789,  0.0143],\n",
       "        [ 0.1147,  0.1246, -0.0902,  ..., -0.1053, -0.1316,  0.0570],\n",
       "        ...,\n",
       "        [ 0.0820, -0.0803,  0.0340,  ..., -0.0170,  0.1419, -0.0437],\n",
       "        [-0.0375,  0.0196, -0.0634,  ..., -0.0050,  0.1458,  0.0978],\n",
       "        [ 0.0128,  0.0569,  0.0877,  ...,  0.0289,  0.1298,  0.0806]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['Linear_1.weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rube Goldberg strikes again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the parameter naming conventions work if we nest multiple blocks inside each other. For that we first define a function that produces blocks (a block factory, so to speak) and then we combine these inside yet larger blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshit Mittal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2980, -0.2109, -0.0334,  0.1267,  0.1291, -0.3061, -0.0142,  0.1217,\n",
       "          0.0752, -0.0719],\n",
       "        [ 0.3117, -0.2294, -0.0615,  0.1087,  0.1479, -0.2471, -0.0197,  0.1489,\n",
       "          0.0400, -0.0269]], grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    net = nn.Sequential(nn.Linear(16, 32),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(32, 16),\n",
    "                        nn.ReLU())\n",
    "    return net\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module('block'+str(i), block1())\n",
    "    return net    \n",
    "        \n",
    "rgnet = nn.Sequential()\n",
    "rgnet.add_module('model',block2())\n",
    "rgnet.add_module('Last_linear_layer', nn.Linear(16,10))\n",
    "rgnet.apply(init_weights)\n",
    "x = torch.randn(2,16)\n",
    "rgnet(x) # forward computation\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are done designing the network, let's see how it is organized. __state_dict__ provides us with this information, both in terms of naming and in terms of logical structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.state_dict of Sequential(\n",
      "  (model): Sequential(\n",
      "    (block0): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block1): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block2): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block3): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (Last_linear_layer): Linear(in_features=16, out_features=10, bias=True)\n",
      ")>\n",
      "OrderedDict([('model.block0.0.weight', tensor([[-0.2053,  0.1735,  0.2060, -0.0839,  0.2391,  0.3008, -0.2452,  0.0932,\n",
      "         -0.0138,  0.0533,  0.3233,  0.2807,  0.1064, -0.2449,  0.1987,  0.1784],\n",
      "        [ 0.1173,  0.3350,  0.3057,  0.3152, -0.2350, -0.1043,  0.2775,  0.0369,\n",
      "         -0.0397,  0.0333,  0.2639,  0.1690,  0.0129,  0.2732, -0.2932,  0.1671],\n",
      "        [-0.1255, -0.0732, -0.2110, -0.2965, -0.2403,  0.0010, -0.1038, -0.2365,\n",
      "         -0.0703,  0.2415,  0.2266,  0.2421,  0.1902, -0.3390, -0.2574, -0.1648],\n",
      "        [-0.2516, -0.2785,  0.0768, -0.2299,  0.0579, -0.0555,  0.0178, -0.1384,\n",
      "         -0.2537,  0.2312, -0.0674,  0.1494,  0.0419,  0.2357,  0.2012, -0.0965],\n",
      "        [-0.2939, -0.0347, -0.3504,  0.2571, -0.2689,  0.0580,  0.2058,  0.3311,\n",
      "         -0.2535,  0.2901, -0.0058,  0.1201,  0.0377, -0.1577, -0.1872,  0.0748],\n",
      "        [-0.2098, -0.2759,  0.0060,  0.0146, -0.3318,  0.0536,  0.1005,  0.0046,\n",
      "         -0.0506,  0.3429,  0.2847,  0.1149,  0.0016, -0.1900, -0.2483,  0.1016],\n",
      "        [-0.1547,  0.3363,  0.2283, -0.3024, -0.2795, -0.1705, -0.2434,  0.2968,\n",
      "         -0.1686, -0.2103,  0.0217, -0.1841,  0.3197,  0.2026, -0.0001, -0.2093],\n",
      "        [ 0.3177, -0.0962,  0.2801,  0.0270, -0.3241, -0.1917,  0.1012, -0.0492,\n",
      "          0.1586,  0.2097,  0.2422,  0.0204, -0.0151,  0.1495, -0.3163, -0.1629],\n",
      "        [ 0.1509, -0.2642, -0.3445, -0.2245, -0.2935,  0.1318, -0.1001,  0.0523,\n",
      "          0.0446,  0.0760, -0.3010, -0.3417,  0.2413, -0.2338, -0.1903,  0.2758],\n",
      "        [-0.3320,  0.1268,  0.2315, -0.2255, -0.3445, -0.3505, -0.1393, -0.3451,\n",
      "         -0.2609, -0.2944,  0.0044,  0.0794, -0.0568, -0.2472, -0.1518,  0.2572],\n",
      "        [ 0.2732, -0.3448, -0.1212, -0.0930,  0.1443, -0.1285, -0.0928, -0.2897,\n",
      "         -0.1229, -0.3013, -0.0709,  0.1681, -0.1126,  0.0990, -0.2071,  0.0129],\n",
      "        [-0.1951, -0.2302, -0.2910, -0.2146,  0.2912, -0.0034,  0.2594, -0.2971,\n",
      "          0.3159,  0.2969, -0.2847,  0.3053, -0.0703,  0.1063,  0.1462, -0.3104],\n",
      "        [-0.3023, -0.2130, -0.2352, -0.1887, -0.2130, -0.3118, -0.1844, -0.3119,\n",
      "          0.0088, -0.2251,  0.0325, -0.1766,  0.3420, -0.0201,  0.2087,  0.3257],\n",
      "        [ 0.1329,  0.2937,  0.1838,  0.0556, -0.2933,  0.1420, -0.2331, -0.2146,\n",
      "         -0.1533, -0.2685, -0.0899,  0.2039, -0.2374,  0.0612, -0.2429,  0.1703],\n",
      "        [-0.0257, -0.0316, -0.3498, -0.1121, -0.2246, -0.2601, -0.2057, -0.0644,\n",
      "          0.2602, -0.0167, -0.1532, -0.0242,  0.0082,  0.1138,  0.1610,  0.3381],\n",
      "        [-0.2104,  0.3286, -0.0072,  0.1631,  0.1907, -0.3165, -0.2013, -0.3061,\n",
      "         -0.1428,  0.2953,  0.1632, -0.0474,  0.2341, -0.3262, -0.1246, -0.2071],\n",
      "        [-0.1239,  0.2743, -0.2890, -0.1203, -0.0235,  0.2340, -0.2701,  0.2277,\n",
      "          0.2844, -0.1327, -0.2171, -0.0014, -0.0923,  0.3351, -0.0993,  0.0560],\n",
      "        [ 0.2271,  0.1165,  0.0275,  0.2200,  0.2462, -0.0422,  0.0170, -0.0335,\n",
      "          0.0433,  0.1403,  0.2770,  0.2235,  0.0792, -0.1999, -0.0277,  0.2000],\n",
      "        [-0.1051, -0.2641,  0.0177, -0.1425, -0.1808,  0.2223,  0.0769, -0.1766,\n",
      "         -0.0608, -0.1464,  0.2489,  0.1110,  0.1105, -0.1243, -0.2736, -0.3523],\n",
      "        [-0.2499, -0.1578, -0.3135,  0.0381, -0.3193, -0.1875,  0.2190,  0.0413,\n",
      "         -0.0697,  0.0796,  0.1394, -0.0172, -0.3116,  0.2778,  0.1161,  0.2911],\n",
      "        [ 0.0256, -0.1555,  0.0919,  0.2726,  0.2482,  0.2278,  0.1490,  0.0140,\n",
      "          0.0294,  0.3348,  0.2715,  0.2526, -0.0809,  0.1510,  0.1276,  0.3319],\n",
      "        [ 0.1939,  0.1027,  0.0977,  0.1546, -0.2910,  0.0470,  0.2529,  0.2520,\n",
      "         -0.1392, -0.0617,  0.2612, -0.0060, -0.3432,  0.0985, -0.1738,  0.1813],\n",
      "        [ 0.0311,  0.0451, -0.1601, -0.1445, -0.3286,  0.0918, -0.1159,  0.1740,\n",
      "         -0.3026, -0.1921, -0.2627, -0.2472, -0.1245, -0.3116,  0.1204,  0.0508],\n",
      "        [ 0.3404, -0.2271, -0.0669, -0.3161, -0.1541,  0.2367,  0.1909,  0.0723,\n",
      "         -0.3199,  0.2762,  0.2671,  0.0807,  0.2676,  0.0447,  0.0144, -0.0423],\n",
      "        [-0.0898,  0.1690,  0.1763,  0.3099, -0.3452, -0.1393,  0.2044,  0.2775,\n",
      "          0.2973, -0.0392,  0.0333, -0.3238,  0.0717,  0.2253,  0.2746,  0.0500],\n",
      "        [-0.0930,  0.1262, -0.1396,  0.3213, -0.0542,  0.0464, -0.0733,  0.0682,\n",
      "          0.0897,  0.2289, -0.0210, -0.0670, -0.3474,  0.0890, -0.2232, -0.0727],\n",
      "        [-0.1553,  0.1303,  0.1060, -0.3162, -0.0141, -0.2472, -0.2459,  0.1812,\n",
      "          0.1898, -0.0428,  0.3362,  0.1765,  0.0084, -0.2738, -0.2526,  0.1008],\n",
      "        [-0.2784,  0.3464, -0.2341,  0.0838, -0.2424,  0.2613, -0.3370, -0.3280,\n",
      "          0.1253,  0.2420,  0.3343, -0.1105, -0.1005, -0.3506,  0.0292, -0.2148],\n",
      "        [-0.0497,  0.2662,  0.3082,  0.0367, -0.0904, -0.1466,  0.2971, -0.2853,\n",
      "          0.2470, -0.2275, -0.1429, -0.0671,  0.2443,  0.3036,  0.3524, -0.3344],\n",
      "        [-0.2796,  0.2138, -0.3005,  0.1175,  0.1934, -0.0248,  0.2813,  0.1941,\n",
      "         -0.3412, -0.0500, -0.0340,  0.1498, -0.1882,  0.1376,  0.2775, -0.2046],\n",
      "        [-0.0074,  0.2472,  0.1264, -0.0237, -0.3353,  0.2157, -0.2362, -0.0211,\n",
      "         -0.0953,  0.0608,  0.0300,  0.1529, -0.1897, -0.0801,  0.0600, -0.3353],\n",
      "        [-0.3371, -0.1557,  0.2391,  0.2844,  0.0259, -0.2750, -0.1067, -0.0231,\n",
      "          0.0506,  0.2714, -0.2609,  0.0543,  0.3042,  0.2803,  0.0947, -0.1274]])), ('model.block0.0.bias', tensor([-0.1972, -0.0723,  0.1812,  0.2143,  0.0247, -0.2165,  0.1020, -0.0403,\n",
      "        -0.0298, -0.0006, -0.1000, -0.1148,  0.1413,  0.1926,  0.1113, -0.1686,\n",
      "         0.1766, -0.2192,  0.2058,  0.0398, -0.0292, -0.2183, -0.0100,  0.2345,\n",
      "        -0.1221, -0.0594,  0.0352, -0.0983,  0.2331, -0.0903,  0.2364,  0.1485])), ('model.block0.2.weight', tensor([[-0.0051,  0.1470, -0.0029,  0.0574,  0.3077,  0.1232, -0.2477,  0.3521,\n",
      "         -0.3348, -0.2603, -0.1899, -0.3310, -0.1149,  0.2245,  0.3093, -0.2316,\n",
      "         -0.2558,  0.2175, -0.0833, -0.2040, -0.2405,  0.2705, -0.0667, -0.1058,\n",
      "         -0.0661,  0.1386,  0.0077,  0.2309, -0.3366,  0.3408, -0.2564,  0.0601],\n",
      "        [ 0.2412,  0.0293,  0.3039, -0.1001, -0.2410,  0.2451, -0.0046, -0.2643,\n",
      "         -0.0286, -0.2500,  0.1761,  0.0612,  0.3230, -0.1950,  0.1383,  0.1887,\n",
      "          0.1275, -0.3415,  0.3189, -0.3510,  0.1176,  0.1875,  0.2440,  0.0920,\n",
      "         -0.0909,  0.2332,  0.3521, -0.1459,  0.2830,  0.0998,  0.0233,  0.2695],\n",
      "        [ 0.1549,  0.0432, -0.3452,  0.1010,  0.0166, -0.0558,  0.3529, -0.0035,\n",
      "         -0.1715, -0.1569,  0.0160, -0.1456, -0.2965, -0.1743,  0.0449, -0.0826,\n",
      "          0.0158, -0.2926, -0.2414, -0.2001, -0.2330, -0.0141,  0.2728,  0.2663,\n",
      "         -0.3107, -0.3519,  0.0588, -0.0113,  0.3248, -0.0302,  0.0952,  0.2211],\n",
      "        [-0.1905, -0.2789,  0.0789,  0.0335,  0.2285,  0.3026,  0.3387, -0.3199,\n",
      "         -0.3432,  0.0415,  0.0861,  0.2033,  0.1854,  0.1539,  0.2288, -0.0618,\n",
      "         -0.3441, -0.2122, -0.1983, -0.0743, -0.1064, -0.1377,  0.3523,  0.2894,\n",
      "          0.3399,  0.3162,  0.2346,  0.2592,  0.2547,  0.3077, -0.1296, -0.2296],\n",
      "        [-0.0159,  0.1705,  0.2746, -0.2956,  0.1485, -0.3468,  0.0261, -0.1278,\n",
      "         -0.2855,  0.1518, -0.1538,  0.0895, -0.0614,  0.1338,  0.1740, -0.2308,\n",
      "          0.0380, -0.2970, -0.1111,  0.0362,  0.2798,  0.2678,  0.1910,  0.1961,\n",
      "          0.1928, -0.2444,  0.3396, -0.1255, -0.3096, -0.1144, -0.0777, -0.3027],\n",
      "        [-0.0120,  0.2235,  0.0782, -0.1133, -0.3048,  0.0914,  0.0997, -0.2163,\n",
      "         -0.1629,  0.2990, -0.0846,  0.1821, -0.1349, -0.2036,  0.0095,  0.2920,\n",
      "         -0.3054,  0.0467, -0.2979, -0.0424,  0.1961,  0.0389, -0.2879,  0.0630,\n",
      "         -0.2749, -0.1401,  0.3262, -0.0848,  0.0609, -0.0994,  0.3430, -0.2020],\n",
      "        [ 0.1081, -0.1047, -0.1996, -0.1126, -0.2490, -0.1973, -0.1005, -0.3106,\n",
      "         -0.3523,  0.3417, -0.0795,  0.2665,  0.1527,  0.0188,  0.0310,  0.2527,\n",
      "          0.1102, -0.3366,  0.0261,  0.0493,  0.2948, -0.1378,  0.1843, -0.2814,\n",
      "          0.1800,  0.2564, -0.2665, -0.0968,  0.0985,  0.2372,  0.3068, -0.2043],\n",
      "        [-0.2776,  0.0861,  0.0075, -0.0380,  0.1289,  0.0901, -0.0731,  0.0184,\n",
      "          0.1211, -0.0676,  0.3464,  0.0639, -0.1630,  0.1833,  0.1189, -0.2360,\n",
      "         -0.3011,  0.1672, -0.1787,  0.0906,  0.1305,  0.2046, -0.3375,  0.1161,\n",
      "         -0.2008,  0.2503,  0.0959,  0.1073,  0.0546, -0.1405, -0.2995, -0.1863],\n",
      "        [ 0.0432, -0.0873, -0.1882,  0.3138,  0.0628,  0.3227, -0.2507, -0.1594,\n",
      "          0.1331, -0.0071, -0.1521,  0.0213, -0.0496,  0.3463,  0.1551, -0.1117,\n",
      "         -0.2426, -0.0452, -0.2026, -0.2158, -0.0188, -0.3021, -0.0187,  0.2598,\n",
      "         -0.2557,  0.1922,  0.1733, -0.2225, -0.3533, -0.2627,  0.1077, -0.0025],\n",
      "        [-0.1782,  0.2403, -0.0896, -0.0107,  0.2841,  0.0847, -0.2950, -0.2126,\n",
      "         -0.3438,  0.0833,  0.1482, -0.2428,  0.2086,  0.1879, -0.0791,  0.0614,\n",
      "         -0.1961,  0.0943, -0.1990, -0.3469,  0.0568, -0.0975, -0.2837, -0.2064,\n",
      "         -0.1243, -0.2250, -0.0572, -0.1266,  0.2310,  0.3235, -0.3105,  0.0630],\n",
      "        [ 0.2097,  0.1598, -0.2640, -0.2197,  0.0470, -0.1162,  0.1075, -0.1266,\n",
      "         -0.3021, -0.3023,  0.1734, -0.2069, -0.0195, -0.1609, -0.0273, -0.0445,\n",
      "         -0.2434, -0.3187, -0.2948, -0.3022, -0.0100,  0.0525, -0.0350,  0.3279,\n",
      "          0.0348, -0.1406, -0.0638,  0.2528,  0.3339,  0.2725, -0.2951,  0.3298],\n",
      "        [ 0.2393, -0.0632, -0.1679,  0.2788,  0.1765, -0.3435,  0.1269,  0.3029,\n",
      "         -0.3496,  0.2497,  0.3511,  0.2752, -0.2225, -0.2080, -0.0894,  0.2185,\n",
      "         -0.3486, -0.0857, -0.1959,  0.2077, -0.0343,  0.3120, -0.1366,  0.2137,\n",
      "          0.2113,  0.0905,  0.2613, -0.0261,  0.3114, -0.1955, -0.1213,  0.1326],\n",
      "        [-0.2319,  0.2796,  0.0624,  0.0743,  0.1989, -0.1926,  0.3067,  0.2089,\n",
      "         -0.3347, -0.1298,  0.1713, -0.1630, -0.3060,  0.1450,  0.1856,  0.3472,\n",
      "         -0.3388,  0.1584,  0.0030, -0.1907,  0.1607, -0.1321, -0.1413,  0.0925,\n",
      "          0.3198,  0.1457, -0.1511, -0.1204,  0.2268,  0.3486,  0.2784, -0.0999],\n",
      "        [-0.2219, -0.0249, -0.1558, -0.0138,  0.2879,  0.1662, -0.0196, -0.0466,\n",
      "          0.1839,  0.2053,  0.1453,  0.2585,  0.2563,  0.1214,  0.0110, -0.1509,\n",
      "         -0.2060, -0.3506, -0.3420, -0.2268,  0.1432,  0.0294,  0.3288,  0.2721,\n",
      "          0.2228,  0.0779,  0.1122, -0.0883,  0.1521,  0.0943,  0.0036, -0.0241],\n",
      "        [-0.2606,  0.1373, -0.0354, -0.0236, -0.2417, -0.2005,  0.3437,  0.3047,\n",
      "         -0.1396, -0.0104,  0.0558,  0.3494, -0.2426, -0.0981, -0.0545, -0.1489,\n",
      "          0.2374,  0.1685, -0.1926,  0.0251, -0.3046,  0.0752, -0.0762, -0.0479,\n",
      "          0.0814, -0.0915,  0.3027, -0.0085,  0.1169, -0.1929,  0.3496,  0.1065],\n",
      "        [ 0.0644,  0.0825, -0.2826,  0.1424,  0.1448,  0.2874,  0.3109,  0.2303,\n",
      "         -0.2359,  0.3335,  0.2073,  0.2486,  0.1899,  0.3101,  0.1725, -0.1013,\n",
      "         -0.0667, -0.2561, -0.0216,  0.1359, -0.1295, -0.0318, -0.0162, -0.2648,\n",
      "         -0.1640, -0.0592, -0.1526,  0.1119,  0.1270,  0.2624,  0.1813, -0.0956]])), ('model.block0.2.bias', tensor([-0.1125,  0.0032, -0.0761,  0.1474,  0.0732,  0.0754, -0.1096,  0.0894,\n",
      "        -0.1017,  0.0754,  0.1757, -0.1499,  0.0123,  0.1064, -0.1168,  0.1457])), ('model.block1.0.weight', tensor([[ 0.3114, -0.1271, -0.3208, -0.1544,  0.0304,  0.0867,  0.0420, -0.1893,\n",
      "         -0.2482, -0.2153, -0.2073, -0.2807,  0.1470, -0.2750,  0.2451,  0.3372],\n",
      "        [-0.2230,  0.1910,  0.3119, -0.1625, -0.2358, -0.2141, -0.2068,  0.3090,\n",
      "         -0.2999, -0.3305, -0.2228, -0.0583,  0.1809,  0.0287, -0.1316, -0.0286],\n",
      "        [ 0.3342,  0.1327,  0.2013,  0.1772, -0.3040,  0.3459, -0.3187, -0.3440,\n",
      "         -0.0966, -0.1842,  0.2003,  0.1362,  0.2343,  0.0042, -0.3529,  0.0774],\n",
      "        [-0.3315, -0.1229,  0.2854,  0.0596, -0.1754,  0.1525, -0.3114, -0.3307,\n",
      "         -0.0181, -0.1999,  0.0848,  0.2166, -0.1798, -0.1718, -0.2298, -0.1214],\n",
      "        [-0.3131,  0.2088,  0.2698,  0.2118,  0.3115,  0.2180,  0.0577, -0.2027,\n",
      "          0.1691, -0.0288, -0.1918,  0.0635,  0.3050, -0.2075, -0.0764, -0.0651],\n",
      "        [-0.0216, -0.1092,  0.3136,  0.3492, -0.2726,  0.1547,  0.1524,  0.2093,\n",
      "         -0.1285,  0.2550, -0.0953, -0.0044, -0.3046,  0.0764, -0.0417,  0.1518],\n",
      "        [-0.1238,  0.3034,  0.1968, -0.1062, -0.1823, -0.2249,  0.1833, -0.0402,\n",
      "          0.2740, -0.1459,  0.3416, -0.0868,  0.2036,  0.1463,  0.0722, -0.2507],\n",
      "        [-0.2057, -0.1721, -0.1625, -0.0998, -0.0611, -0.1248,  0.2498,  0.0218,\n",
      "         -0.0952, -0.2136,  0.1318, -0.0016, -0.0404,  0.0909,  0.0782, -0.3256],\n",
      "        [-0.1319, -0.2557, -0.0637,  0.1007,  0.1432,  0.1360,  0.0062, -0.2460,\n",
      "          0.1247,  0.0565, -0.0256, -0.0863, -0.1093,  0.3294, -0.2658,  0.3086],\n",
      "        [-0.2784, -0.1394,  0.2538, -0.2438,  0.0054, -0.3039, -0.2106,  0.2786,\n",
      "          0.3065, -0.1407,  0.2696,  0.0029,  0.3365,  0.2835, -0.3518,  0.3062],\n",
      "        [-0.2769, -0.2115, -0.3358,  0.2498,  0.2044,  0.2269, -0.2947,  0.2723,\n",
      "         -0.1457,  0.3517, -0.3329, -0.1171, -0.0302,  0.0842,  0.2698, -0.0897],\n",
      "        [-0.2866, -0.0151, -0.0406, -0.1712, -0.1613,  0.1578,  0.3281,  0.1625,\n",
      "          0.0624, -0.2231,  0.1555,  0.1109,  0.0244,  0.0460, -0.0171,  0.0931],\n",
      "        [-0.2802, -0.0908,  0.2154, -0.0173,  0.3006,  0.0048, -0.0137,  0.0539,\n",
      "          0.1502,  0.1425, -0.3500, -0.0049,  0.2492, -0.2202, -0.0235, -0.1417],\n",
      "        [-0.3243,  0.3277, -0.3063,  0.2778,  0.2115,  0.3349, -0.0378, -0.1127,\n",
      "          0.0098,  0.2836, -0.0590, -0.1286,  0.1659, -0.2295, -0.3397, -0.2564],\n",
      "        [ 0.0580, -0.2046, -0.1959, -0.3179, -0.0773,  0.1211,  0.0582, -0.0712,\n",
      "          0.0454,  0.1708, -0.2163,  0.2048,  0.1332,  0.3491,  0.0518,  0.0351],\n",
      "        [-0.0669,  0.2885,  0.3086,  0.3168,  0.0157,  0.3268, -0.3532,  0.2947,\n",
      "          0.2578, -0.2068,  0.2796,  0.2908,  0.3402,  0.3518,  0.0814, -0.1144],\n",
      "        [-0.3246,  0.1403, -0.0311,  0.2579, -0.2719, -0.2152,  0.0517, -0.1477,\n",
      "         -0.0025, -0.1172,  0.3121, -0.1063,  0.0201,  0.2886, -0.3502, -0.0213],\n",
      "        [-0.1957, -0.0780, -0.3274,  0.1406, -0.1553,  0.1859, -0.1763,  0.0506,\n",
      "          0.0693,  0.2075,  0.1363,  0.0458,  0.3294,  0.1357,  0.3360, -0.0323],\n",
      "        [-0.0556, -0.0311, -0.1777,  0.0946, -0.1321,  0.3337, -0.2133, -0.1534,\n",
      "         -0.3046, -0.1489, -0.3053,  0.0826, -0.2326,  0.2860,  0.1956, -0.2328],\n",
      "        [-0.0223,  0.0163,  0.0094, -0.2387,  0.2380, -0.1506, -0.2807, -0.2123,\n",
      "         -0.1190, -0.1254,  0.0236,  0.1060, -0.3155,  0.3202, -0.2862, -0.1660],\n",
      "        [-0.1273,  0.3363,  0.2543,  0.3058,  0.2501, -0.3197,  0.3138,  0.1997,\n",
      "          0.2967,  0.2697, -0.0562,  0.2538,  0.0170, -0.2654, -0.1178, -0.2814],\n",
      "        [ 0.0504,  0.0753, -0.0199, -0.0927, -0.2469,  0.0151,  0.3366,  0.2970,\n",
      "          0.0734,  0.1170, -0.1906, -0.0834,  0.0698,  0.2106,  0.1935, -0.0245],\n",
      "        [-0.3510,  0.2211,  0.1820,  0.2390, -0.3387,  0.1213, -0.2527, -0.1415,\n",
      "          0.2410,  0.3136, -0.0913,  0.0988, -0.2552,  0.0640, -0.3288,  0.2430],\n",
      "        [-0.0369, -0.0933,  0.0666,  0.2004, -0.1890, -0.2811, -0.3158, -0.2016,\n",
      "          0.0968, -0.0432,  0.2352, -0.1889,  0.2348,  0.0921, -0.2518, -0.0995],\n",
      "        [-0.0391, -0.2863, -0.0335,  0.1710,  0.1589, -0.0043,  0.3295, -0.1737,\n",
      "          0.0518, -0.1292, -0.2613, -0.0222,  0.1994, -0.3128, -0.2701,  0.3010],\n",
      "        [ 0.1544,  0.3480, -0.2081, -0.1032, -0.1052, -0.2765,  0.1395, -0.3413,\n",
      "          0.1479,  0.2029, -0.0910,  0.2891,  0.1863,  0.2805, -0.0687,  0.0354],\n",
      "        [ 0.1235, -0.1192, -0.2165, -0.3429,  0.0220,  0.3393,  0.3269, -0.0819,\n",
      "          0.2120,  0.1287,  0.0754,  0.1797,  0.3281,  0.0182, -0.0556, -0.2598],\n",
      "        [-0.3522,  0.0471,  0.3124, -0.2093, -0.2840,  0.3451, -0.1403,  0.2571,\n",
      "         -0.2775,  0.3000, -0.1486,  0.2938,  0.3028,  0.0155, -0.2815,  0.1627],\n",
      "        [-0.1465, -0.0234, -0.0830, -0.0577, -0.3472, -0.1351, -0.0851, -0.3488,\n",
      "          0.1669, -0.2394, -0.3403,  0.0723,  0.2303, -0.0778,  0.2381,  0.1832],\n",
      "        [ 0.2321, -0.2387,  0.1932, -0.0938,  0.2574, -0.3132,  0.0056, -0.1380,\n",
      "         -0.1819,  0.1345,  0.1429,  0.1477, -0.2524, -0.0536, -0.3445, -0.1309],\n",
      "        [ 0.2928,  0.0870,  0.1556,  0.1723, -0.2776, -0.1258,  0.1756, -0.3010,\n",
      "          0.0748,  0.2617, -0.1754,  0.1347, -0.0423, -0.0068,  0.2354, -0.3146],\n",
      "        [ 0.0066,  0.1431,  0.1035,  0.2362, -0.1899, -0.0283, -0.0364, -0.0399,\n",
      "          0.2780,  0.2026,  0.3037, -0.2502,  0.2555,  0.2320, -0.2040,  0.3392]])), ('model.block1.0.bias', tensor([-0.1081, -0.1655,  0.1439, -0.0012, -0.1704, -0.2026,  0.0021, -0.1666,\n",
      "        -0.0365, -0.0055,  0.1888, -0.1573,  0.1651, -0.1471, -0.0822,  0.1610,\n",
      "         0.0754,  0.0066, -0.0257,  0.2010, -0.0265, -0.0033,  0.0629, -0.0766,\n",
      "        -0.1390, -0.0753, -0.0628,  0.2232,  0.0385, -0.2445, -0.0302,  0.0114])), ('model.block1.2.weight', tensor([[ 0.3491,  0.0384,  0.2116, -0.3117, -0.2987,  0.2997, -0.1308, -0.1529,\n",
      "          0.0963,  0.1026, -0.1185,  0.0562,  0.1422, -0.0410,  0.0358, -0.3083,\n",
      "          0.1776, -0.2866,  0.0049,  0.3103, -0.2180,  0.0726,  0.3527,  0.0482,\n",
      "          0.3383,  0.1106, -0.1117,  0.2552, -0.0131,  0.0779,  0.1909, -0.3468],\n",
      "        [ 0.1910, -0.3242, -0.2543,  0.0084, -0.2189,  0.0448,  0.0119, -0.2570,\n",
      "         -0.0145, -0.1716, -0.1664,  0.1809, -0.0733, -0.3387, -0.2428,  0.3346,\n",
      "          0.1738,  0.0291,  0.1922,  0.1729, -0.1541, -0.1647, -0.3129,  0.2913,\n",
      "          0.0634,  0.2119,  0.3483,  0.3360, -0.2240, -0.1399,  0.1022,  0.0476],\n",
      "        [ 0.2669, -0.3121, -0.0254,  0.3370,  0.1482,  0.1793, -0.0087,  0.0483,\n",
      "          0.0160,  0.2185,  0.2081, -0.0518, -0.1844,  0.3152,  0.1913,  0.1506,\n",
      "         -0.2971,  0.3023,  0.1824,  0.2257,  0.1389,  0.2056, -0.1835,  0.0684,\n",
      "         -0.2822,  0.2623, -0.2479,  0.3016, -0.0129,  0.3448, -0.3535, -0.3191],\n",
      "        [-0.0075, -0.2588, -0.1287, -0.0025, -0.0263, -0.0230, -0.0119, -0.0281,\n",
      "         -0.2351,  0.1407,  0.1860, -0.2950, -0.2619, -0.1827,  0.2667,  0.2680,\n",
      "         -0.0111,  0.0107,  0.0109, -0.3169, -0.1961, -0.1365, -0.0855, -0.2501,\n",
      "          0.0962,  0.3369,  0.1087,  0.2552, -0.2314,  0.2851, -0.3481, -0.3360],\n",
      "        [-0.2309, -0.3314, -0.0348, -0.1080, -0.2824,  0.1309, -0.0411, -0.1577,\n",
      "          0.3492, -0.0900,  0.3111,  0.2591, -0.0917,  0.0717,  0.1770,  0.1956,\n",
      "          0.2960, -0.2785,  0.2774, -0.1927,  0.0175,  0.1919, -0.2086, -0.1892,\n",
      "          0.0716, -0.0438, -0.2046,  0.3059,  0.2565, -0.0986,  0.2989,  0.2832],\n",
      "        [ 0.1485, -0.2727,  0.2938,  0.1855,  0.2113,  0.1573,  0.2133,  0.0063,\n",
      "         -0.1150, -0.3296, -0.1855,  0.3019,  0.3503,  0.1175,  0.2297, -0.2064,\n",
      "          0.0682,  0.2679, -0.1101, -0.2650, -0.3168, -0.0030,  0.1911,  0.1371,\n",
      "          0.0602,  0.0531,  0.2441, -0.2679,  0.2356,  0.2115, -0.0420,  0.1237],\n",
      "        [-0.2256, -0.3426, -0.1079,  0.0638,  0.3429, -0.1692, -0.1771, -0.3143,\n",
      "         -0.2732, -0.0447,  0.0534, -0.0595, -0.2638, -0.2956,  0.0765, -0.1424,\n",
      "         -0.0965,  0.3462, -0.1256, -0.0959, -0.1980, -0.1835, -0.0566, -0.1772,\n",
      "          0.0457, -0.2828, -0.2959,  0.1588,  0.1853, -0.0111, -0.1596,  0.3235],\n",
      "        [-0.0891,  0.0744,  0.0992,  0.1707,  0.2434, -0.0368,  0.1229, -0.2156,\n",
      "         -0.2632,  0.2503, -0.2667,  0.1872, -0.1477,  0.0172, -0.0145, -0.1699,\n",
      "          0.0834, -0.3494,  0.3189,  0.0125, -0.1720, -0.0049,  0.2448, -0.2537,\n",
      "          0.1769, -0.3326,  0.3254,  0.0469,  0.0391,  0.1458, -0.2254, -0.1906],\n",
      "        [-0.3122,  0.0908, -0.0333, -0.1339, -0.0544, -0.1715, -0.0887,  0.1185,\n",
      "         -0.1784,  0.2420,  0.2423, -0.1026, -0.3278, -0.1823, -0.2352, -0.2788,\n",
      "          0.1165,  0.1073, -0.2406,  0.1532, -0.0325, -0.1170, -0.0180,  0.0489,\n",
      "         -0.2170, -0.0327, -0.2527, -0.0429,  0.3039, -0.2487,  0.0980, -0.2469],\n",
      "        [ 0.3156,  0.0821,  0.0257, -0.1508, -0.1457,  0.1509, -0.0069, -0.0601,\n",
      "         -0.1155, -0.1090,  0.1837, -0.0974, -0.1636, -0.1710,  0.0218,  0.3207,\n",
      "          0.1269,  0.2344, -0.2896, -0.2347, -0.2612,  0.2993,  0.2549, -0.1454,\n",
      "         -0.3110,  0.0690,  0.0266,  0.1518,  0.2218, -0.0255,  0.2885,  0.3334],\n",
      "        [-0.2452,  0.1060,  0.1364,  0.1120,  0.2014,  0.3405, -0.0296, -0.3093,\n",
      "         -0.0654, -0.3269,  0.0350, -0.1280, -0.2895, -0.1074, -0.2879,  0.1989,\n",
      "          0.2635, -0.2295, -0.1150,  0.2733,  0.3328, -0.0479, -0.2723, -0.1258,\n",
      "         -0.1280,  0.1842, -0.2415, -0.1701, -0.1399, -0.3155, -0.0315,  0.1160],\n",
      "        [-0.2540,  0.2462, -0.1113,  0.2752,  0.2913,  0.0721,  0.1910, -0.2670,\n",
      "         -0.2646,  0.2550,  0.0639,  0.2939, -0.2295,  0.1511,  0.2301,  0.3377,\n",
      "          0.0293, -0.0243,  0.1370, -0.1463, -0.3475, -0.0778, -0.3366, -0.1246,\n",
      "          0.3212,  0.0576, -0.1300,  0.2646,  0.2282, -0.0816,  0.0546, -0.0481],\n",
      "        [ 0.2655, -0.2620, -0.2393, -0.3004, -0.2615, -0.2639,  0.0749, -0.1104,\n",
      "         -0.0481, -0.3053, -0.0888,  0.1507, -0.0152,  0.2538,  0.0406,  0.1766,\n",
      "         -0.2181,  0.1432,  0.0725,  0.2539, -0.1922, -0.0002, -0.1284,  0.3197,\n",
      "          0.0103,  0.0834,  0.2160,  0.1047, -0.2218, -0.2469, -0.1165, -0.2122],\n",
      "        [ 0.3408, -0.3344,  0.2679,  0.0945, -0.2484,  0.0177,  0.1891,  0.0981,\n",
      "         -0.1496, -0.0262, -0.3321,  0.2171, -0.1241, -0.2659,  0.0257,  0.2715,\n",
      "         -0.2484,  0.1276,  0.3034,  0.2225, -0.2979,  0.0296,  0.0175, -0.1135,\n",
      "          0.0382,  0.0452, -0.0589, -0.2905,  0.0018, -0.2969,  0.1662,  0.1231],\n",
      "        [-0.0479, -0.1015, -0.1787,  0.0476,  0.0372,  0.2692, -0.1998,  0.3039,\n",
      "         -0.3251,  0.0955,  0.0143, -0.0040,  0.2800, -0.3046,  0.2653,  0.3533,\n",
      "         -0.1047, -0.0698, -0.2197, -0.2992,  0.2548, -0.1577, -0.0873, -0.1561,\n",
      "          0.3050,  0.1466,  0.2829,  0.0303,  0.1317,  0.1148,  0.0861, -0.1638],\n",
      "        [-0.0396,  0.1536, -0.0053,  0.0403,  0.2123, -0.0752,  0.2443, -0.1508,\n",
      "         -0.3434, -0.3134,  0.2521,  0.0693, -0.1307,  0.2101,  0.1385,  0.3510,\n",
      "         -0.1143,  0.0497,  0.2410,  0.2035,  0.2495,  0.1976,  0.0109,  0.1542,\n",
      "          0.1684, -0.0802,  0.1257,  0.0069, -0.0786,  0.2492,  0.2007, -0.3148]])), ('model.block1.2.bias', tensor([-0.1274, -0.0546, -0.0092, -0.1224, -0.0551,  0.1008,  0.1567, -0.0278,\n",
      "        -0.1396,  0.0982,  0.1739,  0.0718, -0.0879, -0.1715, -0.0389,  0.0434])), ('model.block2.0.weight', tensor([[ 0.2226,  0.1282,  0.1053,  0.2789, -0.3317,  0.1165, -0.1552, -0.1973,\n",
      "          0.0097, -0.3491,  0.0744, -0.3119, -0.0380, -0.0278,  0.2982,  0.1120],\n",
      "        [-0.3328,  0.2794, -0.1413, -0.0279,  0.2935, -0.2031, -0.0737,  0.3480,\n",
      "          0.1185, -0.0254, -0.0459,  0.3007, -0.1588,  0.2928,  0.2383, -0.2083],\n",
      "        [ 0.1092, -0.1809, -0.0399, -0.0045, -0.1976, -0.3418,  0.0500, -0.0206,\n",
      "          0.1545,  0.1999, -0.0287,  0.2914, -0.1914,  0.0743,  0.2136, -0.2960],\n",
      "        [ 0.2705, -0.3014,  0.3105,  0.1160, -0.0976,  0.0451, -0.0404, -0.0517,\n",
      "         -0.1409,  0.2083,  0.0037, -0.1314,  0.1700, -0.2479,  0.2470, -0.2119],\n",
      "        [-0.2792, -0.2217,  0.1474, -0.3516, -0.0395,  0.0975, -0.2668, -0.2819,\n",
      "          0.3394,  0.0891, -0.1892,  0.0251,  0.1718,  0.0575, -0.0737, -0.1998],\n",
      "        [-0.0226, -0.0562, -0.3016,  0.2609,  0.3468, -0.0124, -0.1789,  0.0817,\n",
      "         -0.2441,  0.1885, -0.0456,  0.2746,  0.0361, -0.3379, -0.1922,  0.2449],\n",
      "        [-0.1278, -0.2633, -0.0645,  0.3007,  0.1658,  0.0179, -0.0465, -0.3222,\n",
      "         -0.1868, -0.0408,  0.1822,  0.0751, -0.3431,  0.3391, -0.2192, -0.2024],\n",
      "        [-0.1658, -0.2318, -0.0187, -0.1745, -0.3378,  0.2850,  0.3295,  0.3366,\n",
      "          0.2566, -0.1285,  0.0248,  0.1654, -0.0483,  0.3280,  0.0105,  0.2138],\n",
      "        [-0.0417,  0.0079,  0.1282,  0.2475, -0.2994,  0.1792, -0.2907, -0.0695,\n",
      "          0.1368, -0.2604,  0.3495,  0.3256,  0.2598,  0.0733,  0.3070,  0.3076],\n",
      "        [ 0.2677, -0.1880,  0.1505, -0.1646,  0.0632, -0.0120,  0.1501, -0.2709,\n",
      "          0.0969, -0.2930,  0.1678, -0.0194, -0.1646,  0.2740,  0.0435,  0.3376],\n",
      "        [ 0.2379,  0.2227, -0.1805, -0.0992, -0.2633,  0.2817, -0.1633, -0.2889,\n",
      "         -0.0589,  0.2410,  0.0291, -0.0769,  0.0759, -0.1143,  0.2911, -0.0043],\n",
      "        [ 0.2342, -0.2642,  0.1261,  0.0623,  0.1417,  0.2189, -0.2871,  0.2909,\n",
      "          0.2501, -0.0081,  0.2291,  0.0019,  0.1757, -0.2568, -0.0900,  0.2202],\n",
      "        [ 0.3377,  0.0414, -0.2447, -0.0155, -0.3373,  0.1623,  0.3359, -0.3291,\n",
      "          0.2204,  0.3129,  0.2757, -0.1936, -0.3529, -0.2925,  0.0565,  0.1535],\n",
      "        [-0.3039, -0.0889,  0.0361, -0.3521,  0.2412, -0.0721, -0.0131, -0.2719,\n",
      "         -0.2926, -0.1709,  0.2015, -0.2967,  0.1991,  0.1932,  0.2717,  0.1915],\n",
      "        [ 0.2091,  0.1464, -0.1829, -0.3129,  0.1475, -0.3035,  0.3039, -0.1089,\n",
      "         -0.1143,  0.0688, -0.2105, -0.2934,  0.1195,  0.1518,  0.2940, -0.0752],\n",
      "        [ 0.0709, -0.0628,  0.2120,  0.2209,  0.2208,  0.1614, -0.2658,  0.0732,\n",
      "          0.1502,  0.3380, -0.2703,  0.2706,  0.0777, -0.1512,  0.0360, -0.2649],\n",
      "        [ 0.3267,  0.2525,  0.1556,  0.1960, -0.0796, -0.0488, -0.0532, -0.3017,\n",
      "          0.2993, -0.1168,  0.2183, -0.2239,  0.2471,  0.0446, -0.2231, -0.3332],\n",
      "        [ 0.3014, -0.0218, -0.1697, -0.2222, -0.2199,  0.0181, -0.0665,  0.2464,\n",
      "          0.0311,  0.3058, -0.1288, -0.0642,  0.2274, -0.2931, -0.0262, -0.1163],\n",
      "        [ 0.1743, -0.2437,  0.0201, -0.1901,  0.3080,  0.2273,  0.1229, -0.2097,\n",
      "         -0.0737,  0.1771,  0.2230,  0.0633,  0.0493,  0.2283, -0.1574,  0.1587],\n",
      "        [-0.2618,  0.1480, -0.2363,  0.0865, -0.0141, -0.3152, -0.3109, -0.1853,\n",
      "          0.0612, -0.0096, -0.0755, -0.0165,  0.0731, -0.0138, -0.2085,  0.1192],\n",
      "        [ 0.3189, -0.1295,  0.0222,  0.2259,  0.2457, -0.0942,  0.0978,  0.1396,\n",
      "         -0.0558, -0.1596, -0.0883, -0.2865,  0.1242,  0.2328,  0.1219,  0.1433],\n",
      "        [ 0.0108,  0.2708,  0.1273, -0.2663,  0.1377,  0.1643,  0.3519, -0.0236,\n",
      "         -0.0658,  0.2461, -0.3480,  0.1151, -0.0128, -0.3483, -0.3294,  0.1383],\n",
      "        [-0.1911,  0.0192, -0.0689,  0.2911,  0.1934,  0.1505,  0.3019, -0.2783,\n",
      "         -0.1099,  0.2569,  0.3344, -0.3057,  0.3521, -0.1156, -0.1160, -0.3037],\n",
      "        [-0.3013, -0.1137, -0.2084, -0.2364, -0.2969, -0.1658, -0.3326, -0.0489,\n",
      "         -0.2241,  0.1399, -0.0378, -0.3140, -0.2810, -0.1263,  0.1327,  0.0588],\n",
      "        [-0.2893, -0.2815,  0.3446, -0.0511, -0.3375, -0.3156,  0.0460,  0.0617,\n",
      "         -0.1171, -0.2520, -0.0839, -0.1185, -0.2506, -0.3271, -0.1854,  0.3478],\n",
      "        [ 0.1260, -0.2714,  0.2329, -0.0949,  0.1838, -0.0705, -0.0807,  0.3184,\n",
      "          0.1699,  0.0412, -0.0352, -0.2551, -0.3424, -0.2577,  0.1595,  0.1396],\n",
      "        [ 0.0456,  0.0114, -0.1619,  0.0313, -0.2693, -0.1015,  0.0733,  0.0711,\n",
      "          0.1422, -0.2657,  0.1179, -0.1823,  0.1302,  0.0766, -0.3459, -0.2297],\n",
      "        [ 0.1827, -0.2718,  0.1482, -0.3132,  0.1764, -0.0216,  0.2536,  0.3190,\n",
      "         -0.2008,  0.0693,  0.0438,  0.0334, -0.2724, -0.0855,  0.3075,  0.1072],\n",
      "        [ 0.0959, -0.2742, -0.0142,  0.2848, -0.2354, -0.1033,  0.0037,  0.0088,\n",
      "         -0.2599,  0.1480,  0.1457, -0.1505, -0.2753, -0.3530,  0.2698, -0.1909],\n",
      "        [ 0.0053,  0.2661, -0.0129, -0.1830, -0.1442,  0.0640,  0.0245, -0.1956,\n",
      "          0.1127, -0.1186,  0.2772, -0.2633,  0.0895,  0.1792,  0.2796, -0.1116],\n",
      "        [ 0.1956, -0.1339,  0.2564,  0.2953, -0.1698, -0.1432,  0.0830, -0.1867,\n",
      "         -0.0366,  0.1519,  0.1238, -0.3334,  0.3068,  0.0201, -0.1850, -0.1780],\n",
      "        [ 0.0245,  0.2130, -0.2930, -0.2865, -0.3017, -0.0821, -0.1178, -0.0542,\n",
      "         -0.1158, -0.0843,  0.2926, -0.1484, -0.2877, -0.0979,  0.0777, -0.2391]])), ('model.block2.0.bias', tensor([ 0.0006, -0.1053, -0.1090, -0.1489,  0.0566, -0.2216, -0.2276, -0.2472,\n",
      "        -0.2291, -0.0735, -0.1882,  0.0400, -0.1641, -0.1592, -0.0070, -0.2264,\n",
      "        -0.0228,  0.1801, -0.0905,  0.1983,  0.1209, -0.0984,  0.2345, -0.1319,\n",
      "        -0.2247, -0.0441,  0.2270,  0.0077, -0.2207, -0.1835, -0.0058,  0.1035])), ('model.block2.2.weight', tensor([[ 0.3192, -0.3288,  0.2153,  0.1320,  0.0312,  0.1240,  0.2745, -0.1415,\n",
      "          0.0171, -0.0009,  0.3073, -0.3440, -0.0023, -0.0024,  0.0522, -0.0015,\n",
      "          0.1308,  0.2092, -0.2443, -0.1726,  0.1373, -0.0239,  0.2452, -0.0780,\n",
      "         -0.0425, -0.2969, -0.0289,  0.0896,  0.0860,  0.0554,  0.1652, -0.1283],\n",
      "        [-0.0947, -0.0746, -0.2934,  0.1124, -0.3471,  0.3189, -0.2840,  0.1584,\n",
      "          0.0395,  0.1585, -0.2634,  0.0000,  0.0886,  0.1069,  0.2579, -0.2854,\n",
      "          0.0211,  0.1752,  0.2910, -0.2059,  0.0777, -0.0558,  0.0720,  0.1356,\n",
      "         -0.1874,  0.1184,  0.1933, -0.0140,  0.2976,  0.3421,  0.2169,  0.1445],\n",
      "        [-0.0177, -0.0364, -0.0954, -0.0948, -0.0680, -0.1193, -0.0031, -0.3444,\n",
      "          0.1814,  0.0750,  0.3212, -0.3413, -0.2677, -0.0298, -0.1493,  0.2620,\n",
      "          0.1658, -0.2844, -0.1908,  0.1950,  0.3416, -0.1900, -0.1812,  0.0304,\n",
      "          0.0085, -0.1722,  0.1076,  0.1661,  0.0162, -0.2451,  0.0856, -0.1013],\n",
      "        [-0.2251, -0.0931,  0.1016, -0.0591, -0.2277,  0.2228, -0.0730, -0.2823,\n",
      "          0.2181, -0.1323,  0.2972,  0.0373,  0.0827, -0.2031, -0.2697,  0.0520,\n",
      "          0.2970,  0.0079, -0.2162,  0.1069, -0.1393,  0.1696,  0.1475,  0.0968,\n",
      "          0.0599,  0.0660,  0.3480, -0.0631, -0.3377, -0.1125,  0.0968, -0.1336],\n",
      "        [-0.0282, -0.2603, -0.0906,  0.1543, -0.0964, -0.2071,  0.3335,  0.0350,\n",
      "          0.1630,  0.1246,  0.0594, -0.0721,  0.3457, -0.3486,  0.3337,  0.0337,\n",
      "          0.0044, -0.2625, -0.3480,  0.1825,  0.2845, -0.1896, -0.0389, -0.3294,\n",
      "          0.2280,  0.3348,  0.1331,  0.2160, -0.3345,  0.2725, -0.1465,  0.3304],\n",
      "        [-0.1211, -0.2924,  0.2846, -0.1168, -0.1707,  0.2502,  0.2658, -0.0150,\n",
      "          0.2415, -0.0656,  0.0540, -0.1541,  0.3312, -0.2610, -0.0697, -0.3417,\n",
      "         -0.3121, -0.2776,  0.1363, -0.1732,  0.1012,  0.0280, -0.0936, -0.2682,\n",
      "         -0.3285,  0.0250,  0.3330,  0.0898,  0.0350,  0.1623,  0.1210,  0.3215],\n",
      "        [-0.3194, -0.2906, -0.1687, -0.2826, -0.0213,  0.0355, -0.2309, -0.2263,\n",
      "          0.1995, -0.1453,  0.0201, -0.2913, -0.1003,  0.0742,  0.0570, -0.1526,\n",
      "         -0.2155,  0.1828,  0.0665,  0.2016, -0.0931,  0.2001,  0.3050, -0.1813,\n",
      "         -0.0666,  0.1134,  0.3307,  0.3356, -0.1956,  0.1889,  0.2709,  0.2129],\n",
      "        [-0.1364, -0.0385,  0.3259, -0.0964, -0.2942, -0.2531, -0.2211, -0.0377,\n",
      "         -0.3171,  0.3061,  0.2345,  0.1859, -0.0691,  0.1729,  0.2779,  0.3388,\n",
      "         -0.2596,  0.0082,  0.0725,  0.2265,  0.3365,  0.3203,  0.1136,  0.1378,\n",
      "          0.2378, -0.0000,  0.0559,  0.1486, -0.0701, -0.0744,  0.1198, -0.2520],\n",
      "        [-0.0125, -0.0477, -0.2733,  0.1118,  0.2901,  0.1540, -0.2546, -0.3120,\n",
      "          0.2165, -0.1581, -0.2286, -0.0768, -0.0802,  0.2731,  0.3376,  0.0296,\n",
      "         -0.0999, -0.2879,  0.3011,  0.2997,  0.1792,  0.0582, -0.3045, -0.3014,\n",
      "         -0.1092, -0.1943,  0.3474,  0.3317, -0.0854,  0.1937,  0.2707,  0.2077],\n",
      "        [ 0.0089,  0.3206, -0.3423, -0.0329,  0.0332, -0.0821,  0.0330,  0.2972,\n",
      "          0.0325, -0.1988,  0.2349,  0.0723,  0.0527, -0.2758,  0.2422, -0.2066,\n",
      "         -0.2760,  0.2313,  0.1347, -0.1408, -0.1976,  0.2364, -0.3486, -0.2734,\n",
      "         -0.2283, -0.0385, -0.3096,  0.0059,  0.1566, -0.2699, -0.1149,  0.0554],\n",
      "        [-0.0217, -0.2070,  0.2631,  0.0788,  0.0811, -0.1436, -0.2777, -0.2788,\n",
      "          0.0560,  0.2038, -0.2911, -0.0211,  0.1624, -0.1910, -0.0006, -0.3318,\n",
      "         -0.1711, -0.3465, -0.3167,  0.0957, -0.3351,  0.2932, -0.3409, -0.0360,\n",
      "         -0.0969, -0.3036,  0.1205,  0.2146,  0.0957,  0.0668, -0.1169,  0.2769],\n",
      "        [ 0.2172, -0.2441,  0.0961, -0.1620,  0.2250, -0.1248, -0.3179,  0.3369,\n",
      "          0.1882,  0.0378,  0.1620, -0.2734,  0.1686,  0.3380,  0.0466,  0.0068,\n",
      "          0.1990,  0.2302, -0.2472,  0.2095, -0.2235,  0.3089,  0.2764, -0.1257,\n",
      "         -0.1892,  0.0548,  0.1486, -0.1962, -0.1998, -0.1130,  0.2877,  0.2067],\n",
      "        [-0.1658, -0.1778,  0.2275,  0.1627,  0.1900,  0.1451,  0.1591, -0.0678,\n",
      "          0.1454,  0.1010, -0.0479, -0.1284, -0.2878,  0.0440,  0.2300, -0.3280,\n",
      "          0.1332,  0.0877, -0.1337,  0.1576, -0.0439, -0.2693,  0.3510,  0.1458,\n",
      "         -0.0417, -0.1988, -0.3439, -0.3302, -0.0914, -0.0144,  0.0488, -0.2453],\n",
      "        [ 0.2724, -0.2826,  0.0749,  0.2037, -0.2403, -0.3178,  0.0171,  0.0234,\n",
      "         -0.0902,  0.1437, -0.0923, -0.1639,  0.0362, -0.1673, -0.2839, -0.1604,\n",
      "          0.1168, -0.2186,  0.0460,  0.1111,  0.2195, -0.2429, -0.0945,  0.1871,\n",
      "         -0.0349, -0.2087, -0.3178,  0.0204,  0.0656, -0.0463,  0.0808,  0.3347],\n",
      "        [-0.1649, -0.2090, -0.3391, -0.1681, -0.0961, -0.2180, -0.1825, -0.1288,\n",
      "         -0.1396, -0.1579,  0.0187, -0.1087, -0.2827, -0.1922,  0.0000,  0.2877,\n",
      "          0.1266,  0.2939,  0.2919, -0.1178, -0.3267,  0.0174,  0.1863,  0.2849,\n",
      "         -0.1971,  0.1169, -0.0022,  0.2318, -0.2759, -0.1611, -0.1151,  0.3328],\n",
      "        [ 0.0785, -0.1677, -0.3060,  0.1735, -0.0296, -0.2345, -0.0474,  0.1018,\n",
      "          0.2520,  0.2051,  0.2054,  0.1479,  0.2952, -0.2499,  0.2706, -0.2949,\n",
      "          0.3085, -0.1252, -0.0084, -0.1332,  0.2174,  0.3178,  0.2530, -0.1311,\n",
      "          0.1882,  0.2168, -0.0444, -0.1524,  0.0304, -0.2128,  0.0798, -0.1253]])), ('model.block2.2.bias', tensor([-0.1110,  0.0915, -0.1453,  0.0640,  0.1027,  0.0922, -0.0684,  0.1037,\n",
      "        -0.1713,  0.1219,  0.0945,  0.0442, -0.0546,  0.1405, -0.0160, -0.0607])), ('model.block3.0.weight', tensor([[ 0.0251, -0.0935,  0.2657, -0.2714, -0.0212,  0.0391, -0.2231,  0.0205,\n",
      "         -0.2563, -0.3481, -0.1240,  0.1856,  0.1247,  0.2851, -0.0212,  0.2231],\n",
      "        [ 0.3330, -0.2902, -0.2634,  0.0140,  0.2312,  0.2069,  0.3343,  0.0074,\n",
      "         -0.2138, -0.0236,  0.1763,  0.3071, -0.2559,  0.1933,  0.2095, -0.0535],\n",
      "        [ 0.0825, -0.2475, -0.0698, -0.0613,  0.3417,  0.2712,  0.1941, -0.1116,\n",
      "          0.2693,  0.2191, -0.1287,  0.2060,  0.0687,  0.2556,  0.1029,  0.0828],\n",
      "        [-0.2825, -0.3283, -0.0025,  0.1441,  0.0558, -0.2210,  0.0049, -0.0458,\n",
      "         -0.2961,  0.0234, -0.0148,  0.2653, -0.1825,  0.1056,  0.1787,  0.0369],\n",
      "        [-0.0699, -0.1404, -0.1854, -0.0459, -0.1015, -0.2224,  0.1951,  0.2435,\n",
      "          0.0985,  0.1122,  0.2076,  0.2792,  0.2649,  0.0481, -0.2035, -0.1354],\n",
      "        [ 0.3432, -0.0337, -0.0128, -0.0730, -0.1042,  0.0327,  0.2401,  0.1971,\n",
      "         -0.0891,  0.1511,  0.1761,  0.2741,  0.2951,  0.2465, -0.1635, -0.0050],\n",
      "        [ 0.0765,  0.1061, -0.1420, -0.2908, -0.2540, -0.2892, -0.2134,  0.3208,\n",
      "          0.1505, -0.0655,  0.0972, -0.1384,  0.0418,  0.3523,  0.1119,  0.1142],\n",
      "        [ 0.2146, -0.2242,  0.2347, -0.2527,  0.1374, -0.1437,  0.0509, -0.1282,\n",
      "         -0.1253, -0.2424, -0.2119, -0.0133,  0.2646,  0.1457,  0.0243,  0.2440],\n",
      "        [-0.0498,  0.3099,  0.2879, -0.1053,  0.2084, -0.0927, -0.3120, -0.3348,\n",
      "         -0.0355, -0.2262, -0.0088, -0.1728,  0.3086,  0.0989, -0.0072, -0.0079],\n",
      "        [ 0.1787,  0.1564, -0.0241, -0.2535,  0.0496,  0.0735, -0.1497, -0.2606,\n",
      "         -0.0034,  0.3395, -0.2402,  0.0973,  0.2281,  0.1552,  0.3316,  0.1687],\n",
      "        [ 0.1150, -0.0795, -0.0607, -0.3297,  0.0064, -0.0238,  0.3242, -0.2969,\n",
      "          0.1327,  0.1521, -0.2038,  0.0486, -0.1160,  0.0706,  0.1785,  0.1448],\n",
      "        [-0.0530, -0.0672, -0.1843, -0.2674, -0.1119, -0.2112,  0.3453, -0.0217,\n",
      "          0.1768, -0.2474, -0.0748,  0.2662,  0.3384,  0.2114, -0.3498, -0.3452],\n",
      "        [ 0.1383,  0.0332,  0.0282, -0.0138,  0.0355,  0.2633, -0.2037, -0.1967,\n",
      "         -0.3080,  0.3192,  0.2923,  0.3395, -0.2681,  0.0629,  0.2965,  0.2274],\n",
      "        [-0.1366, -0.2670,  0.1043,  0.2049,  0.2063, -0.1826, -0.0937, -0.0496,\n",
      "         -0.2982,  0.0827,  0.0458,  0.2889, -0.2076,  0.1182, -0.3195,  0.0298],\n",
      "        [ 0.1869,  0.0487, -0.2002,  0.0096,  0.1391, -0.1545, -0.1903, -0.1247,\n",
      "         -0.1665,  0.0621, -0.1734, -0.3476,  0.0248,  0.3024,  0.0105,  0.0063],\n",
      "        [ 0.1160, -0.3413, -0.2673, -0.1601, -0.2667,  0.3195, -0.2985, -0.0140,\n",
      "         -0.3148, -0.1209, -0.0536, -0.3518, -0.0901, -0.3044,  0.2093,  0.0147],\n",
      "        [ 0.1930, -0.3076, -0.2050,  0.1209,  0.0072, -0.1645,  0.2344,  0.0828,\n",
      "          0.3054, -0.1698, -0.1568, -0.0614, -0.2518,  0.0205, -0.1581, -0.0172],\n",
      "        [ 0.0879,  0.1340, -0.3090,  0.2161, -0.0532,  0.1348,  0.2115,  0.2747,\n",
      "          0.2056,  0.3376,  0.1912,  0.0623,  0.3228, -0.2557, -0.2553, -0.1012],\n",
      "        [-0.2311, -0.2543,  0.3282,  0.2732,  0.1797, -0.2702, -0.1176,  0.2149,\n",
      "          0.1227,  0.2377, -0.2777,  0.0649,  0.0868, -0.1168, -0.1551, -0.3220],\n",
      "        [ 0.1156,  0.0572, -0.0863, -0.1542, -0.2374, -0.0642,  0.0629, -0.2096,\n",
      "         -0.2279,  0.2209,  0.2754, -0.3013,  0.1412,  0.0965,  0.1520,  0.1271],\n",
      "        [-0.3351, -0.0841,  0.0002,  0.0037,  0.0016, -0.2339, -0.3068,  0.2463,\n",
      "         -0.1257,  0.2331, -0.3079, -0.1337,  0.0587, -0.0686,  0.0071, -0.2410],\n",
      "        [ 0.1967, -0.0153,  0.2511, -0.3392,  0.0245,  0.1093, -0.2316, -0.2711,\n",
      "         -0.1363, -0.1378, -0.1256,  0.0980,  0.2602, -0.2993, -0.2746,  0.1104],\n",
      "        [ 0.3415, -0.1500, -0.2144,  0.2310, -0.3065,  0.1251, -0.1881,  0.3033,\n",
      "          0.2513, -0.1647, -0.0450,  0.2460, -0.1343,  0.1790, -0.1800, -0.3414],\n",
      "        [-0.2758,  0.0556,  0.1467,  0.1348, -0.2648, -0.2603,  0.2499, -0.2293,\n",
      "          0.0916, -0.0560, -0.1894,  0.3298, -0.3130, -0.2503,  0.2290,  0.0718],\n",
      "        [-0.0767, -0.0924,  0.3270,  0.0565,  0.0235,  0.1680, -0.3211, -0.2376,\n",
      "          0.0610,  0.0913, -0.3382, -0.2219, -0.0928, -0.1789,  0.1372,  0.1537],\n",
      "        [ 0.1467, -0.0168,  0.0536,  0.0333, -0.1140, -0.1115,  0.1853, -0.2700,\n",
      "         -0.1174,  0.1112, -0.2627,  0.1646,  0.0326,  0.1700, -0.1292,  0.3452],\n",
      "        [-0.2789, -0.3000, -0.1062, -0.0854, -0.2767, -0.0304,  0.1188,  0.1758,\n",
      "         -0.2937, -0.1523, -0.2166, -0.0632, -0.1350, -0.0775,  0.1393, -0.1804],\n",
      "        [-0.0850, -0.1183, -0.2366, -0.0842,  0.2217, -0.0495,  0.3472, -0.0172,\n",
      "         -0.3329, -0.3270,  0.3333,  0.1131, -0.3354,  0.3215, -0.2325,  0.1148],\n",
      "        [-0.1515,  0.3352, -0.3189, -0.3221, -0.2008,  0.1218,  0.0718, -0.3219,\n",
      "         -0.1668,  0.0637,  0.3181,  0.1690, -0.0880,  0.3306, -0.3095,  0.0286],\n",
      "        [ 0.1936, -0.1014,  0.2425, -0.2352, -0.1270,  0.0213, -0.2632, -0.0872,\n",
      "         -0.2601,  0.2876, -0.1847, -0.1795, -0.2916,  0.0728,  0.1255, -0.2877],\n",
      "        [ 0.0650, -0.1141,  0.1727, -0.2528, -0.2268, -0.3269,  0.0993,  0.0865,\n",
      "          0.0896, -0.3280,  0.2603, -0.1379, -0.0795,  0.2898, -0.0331,  0.3339],\n",
      "        [ 0.1770,  0.3249, -0.2033, -0.2154, -0.2050, -0.2869, -0.0397,  0.2502,\n",
      "          0.2204, -0.3167, -0.1638,  0.2794,  0.2646,  0.0424,  0.1104,  0.2309]])), ('model.block3.0.bias', tensor([ 0.1909,  0.0219, -0.2327,  0.1007,  0.0433, -0.2285,  0.0629, -0.0955,\n",
      "         0.1925, -0.1908, -0.0728,  0.0324,  0.0251, -0.1122,  0.1755,  0.1085,\n",
      "        -0.0315, -0.0763, -0.0359,  0.0173,  0.1629,  0.0336, -0.1082,  0.0051,\n",
      "        -0.1185, -0.1229, -0.0602,  0.1884, -0.2439, -0.0595, -0.1408,  0.0570])), ('model.block3.2.weight', tensor([[-0.1031, -0.0438, -0.1727, -0.0944,  0.1541,  0.2997, -0.2407, -0.1215,\n",
      "          0.1507, -0.2263,  0.2269,  0.0622,  0.0263,  0.2004, -0.2890, -0.1190,\n",
      "          0.2894, -0.3479,  0.2166, -0.0122,  0.0646,  0.1314, -0.1220, -0.3225,\n",
      "          0.1168,  0.3526, -0.2601,  0.2443, -0.2176,  0.2438,  0.3276,  0.0408],\n",
      "        [-0.0050,  0.2412, -0.3323, -0.0236, -0.3532,  0.1609,  0.2800, -0.0222,\n",
      "         -0.3429,  0.2274, -0.2649, -0.0523,  0.0623,  0.0073,  0.2313, -0.0944,\n",
      "          0.1250,  0.0257,  0.2934,  0.0896, -0.3218, -0.0299,  0.2323,  0.1231,\n",
      "          0.2120, -0.1554,  0.0775,  0.1299,  0.1266,  0.1932,  0.0670, -0.3397],\n",
      "        [-0.0017, -0.0340, -0.1197, -0.0470, -0.0408, -0.3330,  0.0583,  0.2174,\n",
      "          0.2305, -0.2913,  0.2762, -0.1782, -0.3442,  0.0135, -0.1962,  0.3098,\n",
      "          0.0040, -0.1967,  0.0153,  0.0840, -0.0608,  0.1269,  0.0556,  0.0712,\n",
      "         -0.0328,  0.2206, -0.0746, -0.2953,  0.0907, -0.2746, -0.0514, -0.0936],\n",
      "        [ 0.1535, -0.3011, -0.3234,  0.2382, -0.0854, -0.2698,  0.0689, -0.2985,\n",
      "          0.1974, -0.1842, -0.2479, -0.2069,  0.2514,  0.0266, -0.1375,  0.1021,\n",
      "          0.2029, -0.3025, -0.3156, -0.2420, -0.0141,  0.3421,  0.3457, -0.0961,\n",
      "          0.2216,  0.0379, -0.2119, -0.1704,  0.0613, -0.0937, -0.0335, -0.2671],\n",
      "        [ 0.1702, -0.0155, -0.2736,  0.2495, -0.0979, -0.2000, -0.2929,  0.0434,\n",
      "         -0.2708,  0.0709, -0.0526, -0.3486, -0.3139, -0.2559,  0.1147, -0.0285,\n",
      "         -0.3517,  0.0550,  0.0388, -0.3256,  0.1021, -0.2968, -0.1123,  0.1571,\n",
      "         -0.1375,  0.2314, -0.0960, -0.2999,  0.0784, -0.2766,  0.0373,  0.2582],\n",
      "        [ 0.0800,  0.0343, -0.2658, -0.1350, -0.3088,  0.3454,  0.0920,  0.1993,\n",
      "         -0.2834,  0.3493, -0.3149,  0.3139, -0.1471,  0.2961, -0.3534, -0.1911,\n",
      "         -0.2615,  0.3434, -0.0067, -0.3101, -0.1260,  0.0560, -0.1225,  0.1772,\n",
      "         -0.1375, -0.3223,  0.3087,  0.2222, -0.0064, -0.3105,  0.1361,  0.2596],\n",
      "        [ 0.2790,  0.2023,  0.3041,  0.0673, -0.0210,  0.2272, -0.1464,  0.1706,\n",
      "         -0.0528,  0.3127, -0.2859,  0.0092, -0.1263,  0.0167, -0.0696, -0.3121,\n",
      "          0.0983, -0.2982, -0.2059, -0.1041, -0.2341,  0.0872,  0.1668,  0.1516,\n",
      "         -0.3235, -0.0061,  0.2166, -0.2704,  0.2792,  0.0712,  0.3386, -0.0086],\n",
      "        [ 0.2845, -0.1348,  0.2821, -0.2336,  0.0436,  0.0512,  0.2136,  0.1372,\n",
      "         -0.0782,  0.0687,  0.2601, -0.1610, -0.0222, -0.0610, -0.1996,  0.1072,\n",
      "         -0.2679, -0.0346, -0.1002,  0.2230, -0.0699, -0.2981, -0.1997, -0.2844,\n",
      "         -0.0701, -0.0533, -0.2195, -0.1251,  0.0209,  0.2022, -0.2405, -0.3244],\n",
      "        [-0.0632,  0.2474, -0.2780,  0.1313, -0.1446,  0.0876, -0.1625, -0.1587,\n",
      "         -0.2406, -0.2036, -0.1719, -0.1224,  0.3350,  0.0261,  0.0900, -0.1095,\n",
      "         -0.3178,  0.1173, -0.1334, -0.2100,  0.2079,  0.0459, -0.2124, -0.2517,\n",
      "          0.0748,  0.0807,  0.0490, -0.1422, -0.3451, -0.0734,  0.2692,  0.3104],\n",
      "        [-0.0491, -0.1703, -0.0951, -0.2147,  0.1437,  0.2800,  0.0506,  0.2978,\n",
      "         -0.3349,  0.1038, -0.2839,  0.1367, -0.2378, -0.1855,  0.1262, -0.0420,\n",
      "          0.3147, -0.1904, -0.1127,  0.0922,  0.0456, -0.1992, -0.2710, -0.2164,\n",
      "          0.2301, -0.1382, -0.0430,  0.0810,  0.2001, -0.0802,  0.0389, -0.0727],\n",
      "        [-0.2517,  0.1521, -0.1025, -0.0704,  0.2083,  0.3219,  0.1508,  0.2421,\n",
      "         -0.0458,  0.1138,  0.1330,  0.2838,  0.2288,  0.1311,  0.0189, -0.1242,\n",
      "          0.1714,  0.1572,  0.1273,  0.0339,  0.1356,  0.3206, -0.3429,  0.0767,\n",
      "         -0.0996, -0.2665, -0.0989,  0.3056,  0.1494, -0.0770, -0.0163, -0.1889],\n",
      "        [-0.3286,  0.3040,  0.2874,  0.1895,  0.2125, -0.1439, -0.1844, -0.0718,\n",
      "          0.1558,  0.2818,  0.2088, -0.1924,  0.3517, -0.0591, -0.2312,  0.3041,\n",
      "          0.2282, -0.0858,  0.0060,  0.3529, -0.3126, -0.2676, -0.3387,  0.2607,\n",
      "          0.1510, -0.1105,  0.2446,  0.3265, -0.1120, -0.2117, -0.2740, -0.1974],\n",
      "        [ 0.3268,  0.0756, -0.3444, -0.1883,  0.3018, -0.3177,  0.0950, -0.1837,\n",
      "         -0.3341, -0.0641, -0.0037,  0.1195,  0.2353,  0.2322, -0.2237,  0.2857,\n",
      "          0.0506,  0.1952,  0.0857,  0.1805,  0.1219, -0.0422,  0.1349,  0.0416,\n",
      "          0.0474, -0.2721, -0.1690, -0.0067, -0.3422,  0.3491,  0.1122, -0.1704],\n",
      "        [ 0.2668,  0.3288,  0.2856, -0.1874,  0.1156,  0.2744,  0.0584, -0.2898,\n",
      "         -0.0269, -0.1796,  0.2350, -0.1187,  0.2044, -0.1859, -0.2728,  0.1152,\n",
      "         -0.2887, -0.3176, -0.1272,  0.1484, -0.2116, -0.2748,  0.2062, -0.0409,\n",
      "         -0.2032,  0.2114,  0.1567, -0.2696, -0.0943,  0.1690, -0.1758, -0.2866],\n",
      "        [ 0.0514,  0.0659, -0.1862,  0.1790,  0.2588, -0.1597, -0.1559, -0.1692,\n",
      "         -0.2205,  0.0667, -0.2175, -0.0026,  0.2353,  0.0659,  0.3210,  0.1114,\n",
      "         -0.3329,  0.1440,  0.0212,  0.2873, -0.2072,  0.1456,  0.1831,  0.0109,\n",
      "          0.0866, -0.0403,  0.1354,  0.1100,  0.0050,  0.1124, -0.0605, -0.0297],\n",
      "        [ 0.3315,  0.1715, -0.0948, -0.0003,  0.1689, -0.1533, -0.3382, -0.2074,\n",
      "         -0.1762, -0.3509, -0.2140,  0.0436,  0.1596, -0.3382, -0.0636, -0.3172,\n",
      "          0.2836,  0.0095, -0.2607, -0.2834,  0.1502,  0.2136, -0.0202,  0.3492,\n",
      "          0.1721,  0.1502, -0.1940,  0.0938,  0.2884,  0.1700,  0.0918,  0.1008]])), ('model.block3.2.bias', tensor([ 0.1105,  0.1642, -0.1141, -0.1277, -0.0991,  0.0983, -0.1358, -0.1041,\n",
      "         0.1594, -0.1754,  0.1559, -0.1351, -0.1312, -0.0658, -0.0684,  0.0816])), ('Last_linear_layer.weight', tensor([[-0.2760,  0.3793, -0.3249,  0.3008, -0.1991, -0.0752, -0.3578,  0.4283,\n",
      "         -0.0514,  0.3860,  0.4742, -0.1767, -0.4258, -0.3855,  0.4584,  0.3796],\n",
      "        [-0.0276, -0.4467, -0.4317,  0.0148, -0.0211, -0.0281, -0.4267, -0.1431,\n",
      "         -0.0096,  0.0970, -0.4526,  0.4183, -0.4331, -0.0780,  0.2522,  0.2434],\n",
      "        [-0.4738,  0.2914,  0.1925, -0.4065, -0.2848,  0.4655, -0.2979,  0.1976,\n",
      "         -0.0829,  0.3009, -0.4574, -0.1275, -0.0535,  0.2016, -0.3018, -0.3189],\n",
      "        [ 0.3448, -0.3912, -0.3725, -0.1908, -0.2822,  0.1817, -0.0751,  0.2481,\n",
      "          0.4075,  0.1409, -0.2778,  0.1384,  0.2254, -0.2202, -0.1217,  0.4068],\n",
      "        [ 0.3724, -0.3153,  0.3664, -0.4236, -0.1597,  0.1722,  0.3339,  0.3945,\n",
      "          0.2192,  0.0777,  0.2404,  0.3845, -0.2787, -0.3938, -0.0964, -0.1011],\n",
      "        [-0.4305, -0.4025, -0.0389,  0.4546, -0.3293, -0.2794,  0.2341,  0.4238,\n",
      "          0.0542, -0.3986,  0.1309, -0.3412,  0.0983,  0.3248, -0.4159, -0.2890],\n",
      "        [-0.4513,  0.2728, -0.0975,  0.2124,  0.3900,  0.2214, -0.1615, -0.1559,\n",
      "         -0.4652, -0.0556,  0.4499, -0.0929, -0.4762,  0.2485,  0.3454,  0.4096],\n",
      "        [-0.4269,  0.2129,  0.2736,  0.3084,  0.2234, -0.0802,  0.1072,  0.3618,\n",
      "          0.1834,  0.1935,  0.3379,  0.3708,  0.3323,  0.2821, -0.1651,  0.3777],\n",
      "        [ 0.2402,  0.1341,  0.4689,  0.0107, -0.0820,  0.3577, -0.3068,  0.4761,\n",
      "          0.4245,  0.0562, -0.2256, -0.2398, -0.3743,  0.0340, -0.3085,  0.4415],\n",
      "        [-0.3311, -0.0673, -0.0221,  0.0338,  0.0396,  0.1852, -0.2995, -0.1619,\n",
      "          0.0452, -0.1792,  0.4206,  0.2824,  0.2301, -0.3549,  0.1337, -0.2602]])), ('Last_linear_layer.bias', tensor([ 0.1512, -0.0882,  0.0374,  0.0317,  0.0279, -0.2017, -0.0791, -0.0292,\n",
      "        -0.1208, -0.1422]))])\n"
     ]
    }
   ],
   "source": [
    "print(rgnet.state_dict)\n",
    "print(rgnet.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the layers are hierarchically generated, we can also access them accordingly. For instance, to access the first major block, within it the second subblock and then within it, in turn the bias of the first layer, we perform the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1081, -0.1655,  0.1439, -0.0012, -0.1704, -0.2026,  0.0021, -0.1666,\n",
       "        -0.0365, -0.0055,  0.1888, -0.1573,  0.1651, -0.1471, -0.0822,  0.1610,\n",
       "         0.0754,  0.0066, -0.0257,  0.2010, -0.0265, -0.0033,  0.0629, -0.0766,\n",
       "        -0.1390, -0.0753, -0.0628,  0.2232,  0.0385, -0.2445, -0.0302,  0.0114])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize the weights of a single layer, we use a function from __torch.nn.init__ . For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0013, -0.0008],\n",
       "        [ 0.0133,  0.0068],\n",
       "        [ 0.0137, -0.0063],\n",
       "        [ 0.0058,  0.0006],\n",
       "        [-0.0147,  0.0046]], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear1 = nn.Linear(2,5,bias=True)\n",
    "torch.nn.init.normal_(linear1.weight, mean=0, std =0.01)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to initialize all parameters to 1, we could do this simply by changing the initializer to Constant(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('Linear_1.weight', tensor([[ 0.0860, -0.4133],\n",
      "        [-0.4154, -1.2724],\n",
      "        [ 0.7713, -2.3440],\n",
      "        [-0.8033, -0.3308],\n",
      "        [ 0.3841,  0.2840]])), ('Linear_2.weight', tensor([[ 0.3219,  0.7539, -0.3076,  0.9368,  0.7773],\n",
      "        [ 0.0464, -0.2074, -1.1464, -0.5490, -0.0038],\n",
      "        [-0.1218,  0.6165, -0.0134, -0.4004, -0.6874],\n",
      "        [ 1.2538,  0.3477, -0.5232, -0.2392, -0.2431],\n",
      "        [-0.1146,  0.6870,  0.4552, -2.1838,  0.3568]]))])\n"
     ]
    }
   ],
   "source": [
    "def init_weight(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.normal_(m.weight)\n",
    "        \n",
    "net = nn.Sequential()\n",
    "net.add_module('Linear_1', nn.Linear(2, 5, bias = False))\n",
    "net.add_module('Linear_2', nn.Linear(5, 5, bias = False))\n",
    "\n",
    "net.apply(init_weight)\n",
    "print(net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s begin with the built-in initializers. The code below initializes all parameters with Gaussian random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.0308, -0.0032],\n",
      "        [ 1.1529,  0.4452],\n",
      "        [-1.0855, -0.8679],\n",
      "        [ 0.1205, -0.8966],\n",
      "        [ 1.0436,  1.8051]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "def gaussian_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.normal_(m.weight)\n",
    "        \n",
    "net.apply(gaussian_normal)\n",
    "print(net[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to initialize all parameters to 1, we could do this simply by changing the initializer to __torch.nn.init.constant_(tensor,1)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('Linear_1.weight', tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])), ('Linear_2.weight', tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]]))])\n"
     ]
    }
   ],
   "source": [
    "def ones(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.constant_(m.weight, 1)\n",
    "        \n",
    "net.apply(ones)\n",
    "print(net.state_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to initialize only a specific parameter in a different manner, we can simply set the initializer only for the appropriate subblock (or parameter) for that matter. For instance, below we initialize the __second layer__ to a constant value of __42__ and we use the __Xavier initializer__ for the weights of the __first layer__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('first.Linear_1.weight', tensor([[ 0.6575,  0.6740],\n",
      "        [ 0.0766, -0.4489],\n",
      "        [-0.3454, -0.3206],\n",
      "        [ 0.8269,  0.7122],\n",
      "        [-0.8657, -0.6929]])), ('second.Linear_2.weight', tensor([[42., 42., 42., 42., 42.],\n",
      "        [42., 42., 42., 42., 42.],\n",
      "        [42., 42., 42., 42., 42.],\n",
      "        [42., 42., 42., 42., 42.],\n",
      "        [42., 42., 42., 42., 42.]]))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshit Mittal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "block1 = nn.Sequential()\n",
    "block1.add_module('Linear_1', nn.Linear(2,5,bias=False))\n",
    "block2 = nn.Sequential()\n",
    "block2.add_module('Linear_2', nn.Linear(5,5,bias=False))\n",
    "\n",
    "model = nn.Sequential()\n",
    "model.add_module('first', block1)\n",
    "model.add_module('second', block2)\n",
    "\n",
    "def xavier_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.constant_(m.weight, 42)\n",
    "              \n",
    "block1.apply(xavier_normal)\n",
    "block2.apply(init_42)\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, the initialization methods we need are not provided in the init module. At this point, we can implement our desired implementation by writing the desired functions and use them to initialize the weights. In the example below, we pick a decidedly bizarre and nontrivial distribution, just to prove the point. We draw the coefficients from the following distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{aligned} w \\sim \\begin{cases} U[5, 10] & \\text{ with probability } \\frac{1}{4} \\   \n",
    "0 & \\text{ with probability } \\frac{1}{2} \\      \n",
    "U[-10, -5] & \\text{ with probability } \\frac{1}{4} \\end{cases} \\end{aligned} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[-6.6177,  0.0000,  6.2309,  5.1063,  0.0000],\n",
      "        [-5.1540,  0.0000,  0.0000,  7.2982,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000, -5.6190, -6.4968],\n",
      "        [ 0.0000,  5.8972,  8.2195,  0.0000, -6.8732],\n",
      "        [ 0.0000,  7.2649, -8.0269, -9.7509,  0.0000]]))])\n"
     ]
    }
   ],
   "source": [
    "def custom(m):\n",
    "    torch.nn.init.uniform_(m[0].weight, -10,10)\n",
    "    for i in range(m[0].weight.data.shape[0]):\n",
    "        for j in range(m[0].weight.data.shape[1]):\n",
    "            if m[0].weight.data[i][j]<=5 and m[0].weight.data[i][j]>=-5:\n",
    "                m[0].weight.data[i][j]=0\n",
    "    \n",
    "    \n",
    "m = nn.Sequential(nn.Linear(5,5,bias=False))\n",
    "custom(m)\n",
    "print(m.state_dict())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If even this functionality is insufficient, we can set parameters directly. Since __.data__ returns a Tensor we can access it just like any other matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42.0000,  1.0000,  7.2309,  6.1063,  1.0000],\n",
       "        [-4.1540,  1.0000,  1.0000,  8.2982,  1.0000],\n",
       "        [ 1.0000,  1.0000,  1.0000, -4.6190, -5.4968],\n",
       "        [ 1.0000,  6.8972,  9.2195,  1.0000, -5.8732],\n",
       "        [ 1.0000,  8.2649, -7.0269, -8.7509,  1.0000]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[0].weight.data +=1\n",
    "m[0].weight.data[0][0] = 42\n",
    "m[0].weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tied Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, we want to share model parameters across multiple layers. For instance when we want to find good word embeddings we may decide to use the same parameters both for encoding and decoding of words. Let’s see how to do this a bit more elegantly. In the following we allocate a linear layer and then use it multiple times for sharing the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshit Mittal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# We need to give the shared layer a name such that we can reference its\n",
    "# parameters\n",
    "\n",
    "shared = nn.Sequential()\n",
    "shared.add_module('linear_shared', nn.Linear(8,8,bias=False))\n",
    "shared.add_module('relu_shared', nn.ReLU())                  \n",
    "net = nn.Sequential(nn.Linear(20,8,bias=False),\n",
    "               nn.ReLU(),\n",
    "               shared,\n",
    "               shared,\n",
    "               nn.Linear(8,10,bias=False))\n",
    "\n",
    "net.apply(init_weights)\n",
    "\n",
    "print(net[2][0].weight==net[3][0].weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example shows that the parameters of the second and third layer are tied. They are identical rather than just being equal. That is, by changing one of the parameters the other one changes, too. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
