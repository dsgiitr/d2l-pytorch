{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Densely Connected Networks (DenseNet)\n",
    "\n",
    "ResNet significantly changed the view of how to parametrize the functions in deep networks. DenseNet\n",
    "is to some extent the logical extension of this. To understand how to arrive at it, let’s take a small detour\n",
    "to theory. Recall the Taylor $3$ expansion for functions. For scalars it can be written as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x) = f(0) + f'(x)x + \\frac{1}{2} f''(x)x^{2} + \\frac{1}{6} f(x)x^{3} + o(x^{3})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Decomposition\n",
    "\n",
    "The key point is that it decomposes the function into increasingly higher order terms. In a similar vein,\n",
    "ResNet decomposes functions into\n",
    "\n",
    "                                f(x) = x + g(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, ResNet decomposes f into a simple linear term and a more complex nonlinear one. What if we\n",
    "want to go beyond two terms? A solution was proposed by [Huang et al, 2016](https://arxiv.org/abs/1608.06993) in the form of DenseNet, an architecture that reported record performance on the ImageNet dataset.\n",
    "![](../img/densenet.svg)\n",
    "Fig. 6.18: The main difference between ResNet (left) and DenseNet (right) in cross-layer connections:\n",
    "use of addition and use of concatenation.\n",
    "\n",
    "The key difference between ResNet and DenseNet is that in the latter case outputs are concatenated rather\n",
    "than added. As a result we perform a mapping from x to its values after applying an increasingly complex\n",
    "sequence of functions.\n",
    "            $$x → [x, f 1 (x), f 2 (x, f 1 (x)), f 3 (x, f 1 (x), f 2 (x, f 1 (x)), . . .]$$\n",
    "            \n",
    "In the end, all these functions are combined in an MLP to reduce the number of features again. In terms of\n",
    "implementation this is quite simple - rather than adding terms, we concatenate them. The name DenseNet\n",
    "arises from the fact that the dependency graph between variables becomes quite dense. The last layer of\n",
    "such a chain is densely connected to all previous layers. The main components that compose a DenseNet\n",
    "are dense blocks and transition layers. The former defines how the inputs and outputs are concatenated,\n",
    "while the latter controls the number of channels so that it is not too large.\n",
    "\n",
    "![](../img/DenseNetDense.svg)\n",
    "                           $$ Fig. 6.19: Dense connections in DenseNet $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Blocks\n",
    "\n",
    "DenseNet uses the modified batch normalization, activation, and convolution architecture of ResNet (see\n",
    "the exercise in the previous section). First, we implement this architecture in the conv_block function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import d2l\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from d2l import train_ch5\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def conv_block(input_channels, num_channels):\n",
    "    layers = []\n",
    "    layers.append(nn.BatchNorm2d(input_channels))\n",
    "    layers.append(nn.ReLU())\n",
    "    layers.append(nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1))\n",
    "    blk = nn.Sequential(*layers)\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dense block consists of multiple conv_block units, each using the same number of output channels.\n",
    "In the forward computation, however, we concatenate the input and output of each block on the channel\n",
    "dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_convs, input_channels, num_channels, **kwargs):\n",
    "        super(DenseBlock, self).__init__(**kwargs)\n",
    "        layer = []\n",
    "        for i in range(num_convs):\n",
    "            layer.append(conv_block((num_channels * i + input_channels), num_channels))\n",
    "        self.net = nn.Sequential(*layer)\n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            # Concatenate the input and output of each block on the channel\n",
    "            # dimension\n",
    "            X = torch.cat((X, Y), dim=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we define a convolution block with two blocks of 10 output channels. When\n",
    "using an input with 3 channels, we will get an output with the 3 + 2 × 10 = 23 channels. The number of\n",
    "convolution block channels controls the increase in the number of output channels relative to the number\n",
    "of input channels. This is also referred to as the growth rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 23, 8, 8])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = DenseBlock(2, 3, 10)\n",
    "X = torch.randn(4, 3, 8, 8)\n",
    "Y = blk(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transition Layers\n",
    "\n",
    "Since each dense block will increase the number of channels, adding too many of them will lead to an\n",
    "excessively complex model. A transition layer is used to control the complexity of the model. It reduces\n",
    "the number of channels by using the 1 × 1 convolutional layer and halves the height and width of the\n",
    "average pooling layer with a stride of 2, further reducing the complexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_block(input_channels, num_channels):\n",
    "    layers = []\n",
    "    layers.append(nn.BatchNorm2d(input_channels))\n",
    "    layers.append(nn.ReLU())\n",
    "    layers.append(nn.Conv2d(input_channels, num_channels, kernel_size=1))\n",
    "    layers.append(nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "    blk = nn.Sequential(*layers)\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a transition layer with 10 channels to the output of the dense block in the previous example. This\n",
    "reduces the number of output channels to 10, and halves the height and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 4, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = transition_block(23, 10)\n",
    "blk(Y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet Model\n",
    "\n",
    "Next, we will construct a DenseNet model. DenseNet first uses the same single convolutional layer and\n",
    "maximum pooling layer as ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "class Reshape(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(-1,1,96,96)\n",
    "\n",
    "layers = []\n",
    "layers.append(Reshape())\n",
    "layers.append(nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3))\n",
    "layers.append(nn.BatchNorm2d(64))\n",
    "layers.append(nn.ReLU())\n",
    "layers.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, similar to the four residual blocks that ResNet uses, DenseNet uses four dense blocks. Similar to\n",
    "ResNet, we can set the number of convolutional layers used in each dense block. Here, we set it to 4,\n",
    "consistent with the ResNet-18 in the previous section. Furthermore, we set the number of channels (i.e.\n",
    "growth rate) for the convolutional layers in the dense block to 32, so 128 channels will be added to each\n",
    "dense block.\n",
    "\n",
    "In ResNet, the height and width are reduced between each module by a residual block with a stride of 2.\n",
    "Here, we use the transition layer to halve the height and width and halve the number of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num_channels: the current number of channels\n",
    "num_channels, growth_rate = 64, 32\n",
    "num_convs_in_dense_blocks = [4, 4, 4, 4]\n",
    "for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "    layers.append(DenseBlock(num_convs, num_channels, growth_rate))\n",
    "    # This is the number of output channels in the previous dense block\n",
    "    num_channels += num_convs * growth_rate\n",
    "    # A transition layer that haves the number of channels is added between\n",
    "    # the dense blocks\n",
    "    if i != len(num_convs_in_dense_blocks) - 1:\n",
    "        layers.append(transition_block(num_channels, num_channels // 2))\n",
    "        num_channels = num_channels // 2\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to ResNet, a global pooling layer and fully connected layer are connected at the end to produce\n",
    "the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.append(nn.BatchNorm2d(num_channels))\n",
    "layers.append(nn.ReLU())\n",
    "layers.append(nn.AdaptiveMaxPool2d((1,1)))\n",
    "layers.append(Flatten())\n",
    "layers.append(nn.Linear(num_channels, 10))\n",
    "\n",
    "net = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition and Training\n",
    "\n",
    "Since we are using a deeper network here, in this section, we will reduce the input height and width from\n",
    "224 to 96 to simplify the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size, device = 0.1, 5, 64, d2l.try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:0\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "net.apply(init_weights)\n",
    "net = net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n",
    "train_ch5(net, train_iter, test_iter, criterion, num_epochs, batch_size, device, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
